{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajithamuthukrishnan/opt/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/Users/rajithamuthukrishnan/opt/anaconda3/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:54: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.4.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.11.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_filenames(path):\n",
    "    files = [] \n",
    "    for filename in os.listdir(path):\n",
    "        if not filename.endswith('.xml'):\n",
    "            continue\n",
    "        filepath = os.path.join(path, filename)\n",
    "        files.append(filepath)\n",
    "    return files\n",
    "\n",
    "def extract_train_chunks():\n",
    "    dataframe_collection = {} \n",
    "    for ctr in range(1,11):\n",
    "        positive_file_path = \"../dataset/2018 train/positive_examples/chunk\"+str(ctr)\n",
    "        negative_file_path = \"../dataset/2018 train/negative_examples/chunk\"+str(ctr)\n",
    "        positive_files = extract_filenames(positive_file_path)\n",
    "        negative_files = extract_filenames(negative_file_path)\n",
    "        files = positive_files + negative_files\n",
    "        data_list = []\n",
    "        for file in files:\n",
    "            if 'positive' in file:\n",
    "                label = 1\n",
    "            elif 'negative' in file:\n",
    "                label = 0\n",
    "            fd = open(file,'r')\n",
    "            data = fd.read()\n",
    "            soup = BeautifulSoup(data,'xml')\n",
    "            subject_id = soup.find('ID')\n",
    "            writings = soup.find_all('WRITING')\n",
    "            title = ''\n",
    "            text = ''\n",
    "            for writing in writings:\n",
    "                title = title + writing.find('TITLE').get_text() + ' '\n",
    "                text = text + writing.find('TEXT').get_text() + ' '\n",
    "                row = [subject_id.get_text(), title, text, label]\n",
    "            data_list.append(row)\n",
    "        chunk_name = 'chunk'+str(ctr)\n",
    "        dataframe_collection[chunk_name] = pd.DataFrame(data_list, columns = ['subject_id', 'title', 'text', 'label'])\n",
    "    return dataframe_collection\n",
    "\n",
    "\n",
    "def extract_test_chunks():\n",
    "    dataframe_collection = {} \n",
    "    for ctr in range(1,11):\n",
    "        file_path = \"../dataset/2018 test/chunk\"+str(ctr)\n",
    "        files = extract_filenames(file_path)\n",
    "        data_list = []\n",
    "        for file in files:\n",
    "            fd = open(file,'r')\n",
    "            data = fd.read()\n",
    "            soup = BeautifulSoup(data,'xml')\n",
    "            subject_id = soup.find('ID')\n",
    "            writings = soup.find_all('WRITING')\n",
    "            title = ''\n",
    "            text = ''\n",
    "            for writing in writings:\n",
    "                title = title + writing.find('TITLE').get_text() + ' '\n",
    "                text = text + writing.find('TEXT').get_text() + ' '\n",
    "                row = [subject_id.get_text(), title, text]\n",
    "            data_list.append(row)\n",
    "        chunk_name = 'chunk'+str(ctr)\n",
    "        dataframe_collection[chunk_name] = pd.DataFrame(data_list, columns = ['subject_id', 'title', 'text'])\n",
    "    return dataframe_collection\n",
    "\n",
    "def stemSentence(sentence):\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    token_words=word_tokenize(sentence)\n",
    "    token_words\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(lemmatizer.lemmatize(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "# Preprocess, encode data (word embeddings) for every chunk\n",
    "def preprocess_data(df):\n",
    "#   TITLE CLEAN\n",
    "    df['title_clean'] = df['title'].loc[df['title'] ==  ' [removed] '] = ' '\n",
    "    df['title_clean'] = df['title'].str.lower()\n",
    "    df['title_clean'] = df['title_clean'].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))  \n",
    "    # remove numbers\n",
    "    df['title_clean'] = df['title_clean'].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n",
    "    # remove duplicate spaces\n",
    "    df['title_clean'] = df['title_clean'].apply(lambda elem: re.sub(' +', ' ', elem))\n",
    "    # remove stop words\n",
    "    df['title_clean'] = df['title_clean'].apply(lambda elem: remove_stopwords(elem))\n",
    "    df['title_clean'] = df['title_clean'].apply(lambda elem: stemSentence(elem))\n",
    "    \n",
    "#   TEXT CLEAN\n",
    "    df['text_clean'] = df['text'].loc[df['title'] ==  ' [removed] '] = ' '\n",
    "    df['text_clean'] = df['text'].str.lower()\n",
    "    df['text_clean'] = df['text_clean'].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))  \n",
    "    # remove numbers\n",
    "    df['text_clean'] = df['text_clean'].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n",
    "    # remove duplicate spaces\n",
    "    df['text_clean'] = df['text_clean'].apply(lambda elem: re.sub(' +', ' ', elem))\n",
    "    # remove stop words\n",
    "    df['text_clean'] = df['text_clean'].apply(lambda elem: remove_stopwords(elem))\n",
    "    df['text_clean'] = df['text_clean'].apply(lambda elem: stemSentence(elem))\n",
    "    \n",
    "    df['final_text'] = df['title_clean'] + df['text_clean']\n",
    "    \n",
    "    final_dataset = pd.DataFrame(df['subject_id'])\n",
    "    final_dataset['text'] = df['title_clean'] + ' ' + df['final_text']\n",
    "    if 'label' in df.columns:\n",
    "        final_dataset['label'] = df['label']\n",
    "    \n",
    "    return final_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Bert Model creation\n",
    "def build_bert_model():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    preprocessing_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3', name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    encoder = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4', trainable=True)\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = outputs['pooled_output']\n",
    "    net = tf.keras.layers.Dense(100, name='features')(net)\n",
    "    net = tf.keras.layers.Dense(50)(net)\n",
    "    net = tf.keras.layers.Dense(10)(net)\n",
    "    net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)\n",
    "    model = tf.keras.Model(text_input, net)\n",
    "    \n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    metrics = tf.metrics.BinaryAccuracy()\n",
    "\n",
    "    model.compile(optimizer='Adagrad',\n",
    "                       loss=loss,\n",
    "                       metrics=metrics)\n",
    "\n",
    "    return model\n",
    "\n",
    "def train(train_df_collection, model):\n",
    "    metrics_list = []\n",
    "    \n",
    "    train_df = train_df_collection['chunk1']\n",
    "        \n",
    "    # Use only first 9 chunks for training and 10th chunk for validation \n",
    "    for chunk in range (1, len(train_df_collection) - 1):\n",
    "            chunk_name = 'chunk'+str(chunk)\n",
    "            train_df = train_df.append(train_df_collection[chunk_name])\n",
    "            \n",
    "    df = preprocess_data(train_df)\n",
    "    X_train = df['text']\n",
    "    Y_train = df['label']\n",
    "        \n",
    "    model.fit(X_train, Y_train, epochs=5)\n",
    "\n",
    "    predictions_probs = model.predict(X_train)\n",
    "    predictions = np.where(predictions_probs > 0.5, 1, 0)\n",
    "\n",
    "    score = f1_score(Y_train, predictions, average='weighted')\n",
    "    metrics_list.append(score)\n",
    "        \n",
    "    print ('F1 Score :',f1_score(Y_train, predictions, average=None))\n",
    "    metrics_df = pd.DataFrame(metrics_list, columns=['F1_score'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def validate(df, model):\n",
    "    df = preprocess_data(df)\n",
    "    X_val = df['text']\n",
    "    Y_val = df['label']\n",
    "    \n",
    "    predictions_probs = model.predict(X_val)\n",
    "    predictions = np.where(predictions_probs > 0.5, 1, 0)\n",
    "\n",
    "    print()\n",
    "    print(classification_report(Y_val, predictions, target_names=['Non-Anorexic', 'Anorexic']))\n",
    "    \n",
    "    \n",
    "def test_model(test_chunk_collection, test_labels, model):\n",
    "    for chunk in range(1,11):\n",
    "        chunk_name = 'chunk'+str(chunk)\n",
    "        chunk_df = test_chunk_collection[chunk_name]\n",
    "\n",
    "        # preprocess, vectorize and predict\n",
    "        clean_df = preprocess_data(chunk_df)\n",
    "\n",
    "        X_test = clean_df.text\n",
    "        \n",
    "        chunk_probs = model.predict(X_test)\n",
    "        chunk_pred = np.where(chunk_probs > 0.5, 1, 0)\n",
    "\n",
    "\n",
    "        # Save prediction\n",
    "        pred_df = pd.DataFrame(chunk_pred, columns=['pred'])\n",
    "        pred_df.pred = pred_df.pred.astype('int')\n",
    "\n",
    "        # save predictions to dataframe\n",
    "        chunks_pred_df = pd.DataFrame(clean_df['subject_id'])\n",
    "        chunks_pred_df['pred'] = pred_df['pred'].values\n",
    "\n",
    "    # Map chunk predictions with truth labels\n",
    "    test_pred_list = []\n",
    "    for sub in chunks_pred_df['subject_id']:\n",
    "        value = test_labels.loc[test_labels['subject_id']==sub]['label'].values[0]\n",
    "        value_list = [sub, value]\n",
    "        test_pred_list.append(value_list)\n",
    "    final_test_pred = pd.DataFrame(test_pred_list, columns=['subject_id', 'label'])    \n",
    "    \n",
    "    # Print classification report\n",
    "    print(classification_report(final_test_pred['label'], chunks_pred_df['pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Data - Chunks 1 - 9\n",
    "train_dataframe_collection = extract_train_chunks()\n",
    "\n",
    "# Validation Data - Chunk 10\n",
    "val_df = train_dataframe_collection['chunk10']\n",
    "\n",
    "# Test Data\n",
    "test_dataframe_collection = extract_test_chunks()\n",
    "test_truth_labels = pd.read_csv('../dataset/2018 test/risk-golden-truth-test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.57575758 3.8       ]\n"
     ]
    }
   ],
   "source": [
    "class_weights = compute_class_weight('balanced', classes=np.array([0,1]), y=val_df['label'])\n",
    "print((class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n",
      "WARNING:tensorflow:From /Users/rajithamuthukrishnan/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/rajithamuthukrishnan/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "bert_clf = build_bert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " text (InputLayer)              [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " preprocessing (KerasLayer)     {'input_mask': (Non  0           ['text[0][0]']                   \n",
      "                                e, 128),                                                          \n",
      "                                 'input_word_ids':                                                \n",
      "                                (None, 128),                                                      \n",
      "                                 'input_type_ids':                                                \n",
      "                                (None, 128)}                                                      \n",
      "                                                                                                  \n",
      " keras_layer (KerasLayer)       {'sequence_output':  109482241   ['preprocessing[0][0]',          \n",
      "                                 (None, 128, 768),                'preprocessing[0][1]',          \n",
      "                                 'default': (None,                'preprocessing[0][2]']          \n",
      "                                768),                                                             \n",
      "                                 'encoder_outputs':                                               \n",
      "                                 [(None, 128, 768),                                               \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768)],                                               \n",
      "                                 'pooled_output': (                                               \n",
      "                                None, 768)}                                                       \n",
      "                                                                                                  \n",
      " features (Dense)               (None, 100)          76900       ['keras_layer[0][13]']           \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 50)           5050        ['features[0][0]']               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 10)           510         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " classifier (Dense)             (None, 1)            11          ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,564,712\n",
      "Trainable params: 109,564,711\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert_clf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "43/43 [==============================] - 527s 12s/step - loss: 0.4087 - binary_accuracy: 0.8618\n",
      "Epoch 2/5\n",
      "43/43 [==============================] - 510s 12s/step - loss: 0.3545 - binary_accuracy: 0.8692\n",
      "Epoch 3/5\n",
      "43/43 [==============================] - 532s 12s/step - loss: 0.3057 - binary_accuracy: 0.8801\n",
      "Epoch 4/5\n",
      "43/43 [==============================] - 539s 13s/step - loss: 0.2849 - binary_accuracy: 0.8925\n",
      "Epoch 5/5\n",
      "43/43 [==============================] - 524s 12s/step - loss: 0.2449 - binary_accuracy: 0.9130\n",
      "43/43 [==============================] - 163s 4s/step\n",
      "F1 Score : [0.8797789 0.5380531]\n"
     ]
    }
   ],
   "source": [
    "bert_offline = train(train_dataframe_collection, bert_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 18s 4s/step\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-Anorexic       0.96      0.81      0.88       132\n",
      "    Anorexic       0.39      0.80      0.52        20\n",
      "\n",
      "    accuracy                           0.81       152\n",
      "   macro avg       0.68      0.81      0.70       152\n",
      "weighted avg       0.89      0.81      0.83       152\n",
      "\n"
     ]
    }
   ],
   "source": [
    "validate(val_df, bert_offline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 40s 4s/step\n",
      "10/10 [==============================] - 40s 4s/step\n",
      "10/10 [==============================] - 41s 4s/step\n",
      "10/10 [==============================] - 40s 4s/step\n",
      "10/10 [==============================] - 41s 4s/step\n",
      "10/10 [==============================] - 41s 4s/step\n",
      "10/10 [==============================] - 41s 4s/step\n",
      "10/10 [==============================] - 42s 4s/step\n",
      "10/10 [==============================] - 41s 4s/step\n",
      "10/10 [==============================] - 42s 4s/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.82      0.88       279\n",
      "           1       0.37      0.71      0.49        41\n",
      "\n",
      "    accuracy                           0.81       320\n",
      "   macro avg       0.66      0.77      0.69       320\n",
      "weighted avg       0.88      0.81      0.83       320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_model(test_dataframe_collection, test_truth_labels, bert_offline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_dev",
   "language": "python",
   "name": "project_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
