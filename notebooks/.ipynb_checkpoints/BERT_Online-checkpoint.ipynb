{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajithamuthukrishnan/opt/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_filenames(path):\n",
    "    files = [] \n",
    "    for filename in os.listdir(path):\n",
    "        if not filename.endswith('.xml'):\n",
    "            continue\n",
    "        filepath = os.path.join(path, filename)\n",
    "        files.append(filepath)\n",
    "    return files\n",
    "\n",
    "def extract_train_chunks():\n",
    "    dataframe_collection = {} \n",
    "    for ctr in range(1,11):\n",
    "        positive_file_path = \"../dataset/2018 train/positive_examples/chunk\"+str(ctr)\n",
    "        negative_file_path = \"../dataset/2018 train/negative_examples/chunk\"+str(ctr)\n",
    "        positive_files = extract_filenames(positive_file_path)\n",
    "        negative_files = extract_filenames(negative_file_path)\n",
    "        files = positive_files + negative_files\n",
    "        data_list = []\n",
    "        for file in files:\n",
    "            if 'positive' in file:\n",
    "                label = 1\n",
    "            elif 'negative' in file:\n",
    "                label = 0\n",
    "            fd = open(file,'r')\n",
    "            data = fd.read()\n",
    "            soup = BeautifulSoup(data,'xml')\n",
    "            subject_id = soup.find('ID')\n",
    "            writings = soup.find_all('WRITING')\n",
    "            title = ''\n",
    "            text = ''\n",
    "            for writing in writings:\n",
    "                title = title + writing.find('TITLE').get_text() + ' '\n",
    "                text = text + writing.find('TEXT').get_text() + ' '\n",
    "                row = [subject_id.get_text(), title, text, label]\n",
    "            data_list.append(row)\n",
    "        chunk_name = 'chunk'+str(ctr)\n",
    "        dataframe_collection[chunk_name] = pd.DataFrame(data_list, columns = ['subject_id', 'title', 'text', 'label'])\n",
    "    return dataframe_collection\n",
    "\n",
    "\n",
    "def extract_test_chunks():\n",
    "    dataframe_collection = {} \n",
    "    for ctr in range(1,11):\n",
    "        file_path = \"../dataset/2018 test/chunk\"+str(ctr)\n",
    "        files = extract_filenames(file_path)\n",
    "        data_list = []\n",
    "        for file in files:\n",
    "            fd = open(file,'r')\n",
    "            data = fd.read()\n",
    "            soup = BeautifulSoup(data,'xml')\n",
    "            subject_id = soup.find('ID')\n",
    "            writings = soup.find_all('WRITING')\n",
    "            title = ''\n",
    "            text = ''\n",
    "            for writing in writings:\n",
    "                title = title + writing.find('TITLE').get_text() + ' '\n",
    "                text = text + writing.find('TEXT').get_text() + ' '\n",
    "                row = [subject_id.get_text(), title, text]\n",
    "            data_list.append(row)\n",
    "        chunk_name = 'chunk'+str(ctr)\n",
    "        dataframe_collection[chunk_name] = pd.DataFrame(data_list, columns = ['subject_id', 'title', 'text'])\n",
    "    return dataframe_collection\n",
    "\n",
    "def stemSentence(sentence):\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    token_words=word_tokenize(sentence)\n",
    "    token_words\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(lemmatizer.lemmatize(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "# Preprocess, encode data (word embeddings) for every chunk\n",
    "def preprocess_data(df):\n",
    "#   TITLE CLEAN\n",
    "    df['title_clean'] = df['title'].loc[df['title'] ==  ' [removed] '] = ' '\n",
    "    df['title_clean'] = df['title'].str.lower()\n",
    "    df['title_clean'] = df['title_clean'].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))  \n",
    "    # remove numbers\n",
    "    df['title_clean'] = df['title_clean'].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n",
    "    # remove duplicate spaces\n",
    "    df['title_clean'] = df['title_clean'].apply(lambda elem: re.sub(' +', ' ', elem))\n",
    "    # remove stop words\n",
    "    df['title_clean'] = df['title_clean'].apply(lambda elem: remove_stopwords(elem))\n",
    "    df['title_clean'] = df['title_clean'].apply(lambda elem: stemSentence(elem))\n",
    "    \n",
    "#   TEXT CLEAN\n",
    "    df['text_clean'] = df['text'].loc[df['title'] ==  ' [removed] '] = ' '\n",
    "    df['text_clean'] = df['text'].str.lower()\n",
    "    df['text_clean'] = df['text_clean'].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))  \n",
    "    # remove numbers\n",
    "    df['text_clean'] = df['text_clean'].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n",
    "    # remove duplicate spaces\n",
    "    df['text_clean'] = df['text_clean'].apply(lambda elem: re.sub(' +', ' ', elem))\n",
    "    # remove stop words\n",
    "    df['text_clean'] = df['text_clean'].apply(lambda elem: remove_stopwords(elem))\n",
    "    df['text_clean'] = df['text_clean'].apply(lambda elem: stemSentence(elem))\n",
    "    \n",
    "    df['final_text'] = df['title_clean'] + df['text_clean']\n",
    "    \n",
    "    final_dataset = pd.DataFrame(df['subject_id'])\n",
    "    final_dataset['text'] = df['title_clean'] + ' ' + df['final_text']\n",
    "    if 'label' in df.columns:\n",
    "        final_dataset['label'] = df['label']\n",
    "    \n",
    "    return final_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Bert Model creation\n",
    "def build_bert_model():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    preprocessing_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3', name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    encoder = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4', trainable=True)\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = outputs['pooled_output']\n",
    "    net = tf.keras.layers.Dense(100, name='features')(net)\n",
    "    net = tf.keras.layers.Dense(50)(net)\n",
    "    net = tf.keras.layers.Dense(10)(net)\n",
    "    net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)\n",
    "    model = tf.keras.Model(text_input, net)\n",
    "    \n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    metrics = tf.metrics.BinaryAccuracy()\n",
    "\n",
    "    init_lr = 3e-5\n",
    "    optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                              num_train_steps=1000,\n",
    "                                              num_warmup_steps=100,\n",
    "                                              optimizer_type='adamw')\n",
    "    model.compile(optimizer=optimizer,\n",
    "                       loss=loss,\n",
    "                       metrics=metrics)\n",
    "\n",
    "    return model\n",
    "\n",
    "def train(train_df_collection, model):\n",
    "    metrics_list = []\n",
    "    \n",
    "    # Use only first 9 chunks for training and 10th chunk for validation \n",
    "    for chunk in range (len(train_df_collection)-1):\n",
    "        chunk_name = 'chunk'+str(chunk+1)\n",
    "        print('Training ' + chunk_name + '...')\n",
    "        df = preprocess_data(train_df_collection[chunk_name])\n",
    "        X_train = df['text']\n",
    "        Y_train = df['label']\n",
    "        \n",
    "        model.fit(X_train, Y_train, epochs=5)\n",
    "        \n",
    "        predictions_probs = model.predict(X_train)\n",
    "        predictions = np.where(predictions_probs > 0.5, 1, 0)\n",
    "     \n",
    "        score = f1_score(Y_train, predictions, average='weighted')\n",
    "        metrics_list.append(score)\n",
    "        \n",
    "        print ('F1 Score :',f1_score(Y_train, predictions, average=None))\n",
    "    metrics_df = pd.DataFrame(metrics_list, columns=['F1_score'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def validate(df, model):\n",
    "    \n",
    "    df = preprocess_data(df)\n",
    "    X_val = df['text']\n",
    "    Y_val = df['label']\n",
    "    \n",
    "    predictions_probs = model.predict(X_val)\n",
    "    predictions = np.where(predictions_probs > 0.5, 1, 0)\n",
    "\n",
    "    print()\n",
    "    print(classification_report(Y_val, predictions, target_names=['Non-Anorexic', 'Anorexic']))\n",
    "    \n",
    "    \n",
    "def test_model(test_chunk_collection, test_labels, model):\n",
    "    \n",
    "    for chunk in range(1,11):\n",
    "        chunk_name = 'chunk'+str(chunk)\n",
    "        chunk_df = test_chunk_collection[chunk_name]\n",
    "\n",
    "        # preprocess, vectorize and predict\n",
    "        clean_df = preprocess_data(chunk_df)\n",
    "\n",
    "        X_test = clean_df.text\n",
    "        \n",
    "        chunk_probs = model.predict(X_test)\n",
    "        chunk_pred = np.where(chunk_probs > 0.5, 1, 0)\n",
    "\n",
    "\n",
    "        # Save prediction\n",
    "        pred_df = pd.DataFrame(chunk_pred, columns=['pred'])\n",
    "        pred_df.pred = pred_df.pred.astype('int')\n",
    "\n",
    "        # save predictions to dataframe\n",
    "        chunks_pred_df = pd.DataFrame(clean_df['subject_id'])\n",
    "        chunks_pred_df['pred'] = pred_df['pred'].values\n",
    "\n",
    "    # Map chunk predictions with truth labels\n",
    "    test_pred_list = []\n",
    "    for sub in chunks_pred_df['subject_id']:\n",
    "        value = test_labels.loc[test_labels['subject_id']==sub]['label'].values[0]\n",
    "        value_list = [sub, value]\n",
    "        test_pred_list.append(value_list)\n",
    "    final_test_pred = pd.DataFrame(test_pred_list, columns=['subject_id', 'label'])    \n",
    "    \n",
    "    # Print classification report\n",
    "    print(classification_report(final_test_pred['label'], chunks_pred_df['pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Data - Chunks 1 - 9\n",
    "train_dataframe_collection = extract_train_chunks()\n",
    "\n",
    "# Validation Data - Chunk 10\n",
    "val_df = train_dataframe_collection['chunk10']\n",
    "\n",
    "# Test Data\n",
    "test_dataframe_collection = extract_test_chunks()\n",
    "test_truth_labels = pd.read_csv('../dataset/2018 test/risk-golden-truth-test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.57575758 3.8       ]\n"
     ]
    }
   ],
   "source": [
    "class_weights = compute_class_weight('balanced', classes=np.array([0,1]), y=val_df['label'])\n",
    "print((class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_clf = build_bert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " text (InputLayer)              [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " preprocessing (KerasLayer)     {'input_word_ids':   0           ['text[0][0]']                   \n",
      "                                (None, 128),                                                      \n",
      "                                 'input_mask': (Non                                               \n",
      "                                e, 128),                                                          \n",
      "                                 'input_type_ids':                                                \n",
      "                                (None, 128)}                                                      \n",
      "                                                                                                  \n",
      " keras_layer_2 (KerasLayer)     {'encoder_outputs':  109482241   ['preprocessing[0][0]',          \n",
      "                                 [(None, 128, 768),               'preprocessing[0][1]',          \n",
      "                                 (None, 128, 768),                'preprocessing[0][2]']          \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768)],                                               \n",
      "                                 'pooled_output': (                                               \n",
      "                                None, 768),                                                       \n",
      "                                 'default': (None,                                                \n",
      "                                768),                                                             \n",
      "                                 'sequence_output':                                               \n",
      "                                 (None, 128, 768)}                                                \n",
      "                                                                                                  \n",
      " features (Dense)               (None, 100)          76900       ['keras_layer_2[0][13]']         \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 50)           5050        ['features[0][0]']               \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 10)           510         ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " classifier (Dense)             (None, 1)            11          ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,564,712\n",
      "Trainable params: 109,564,711\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert_clf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training chunk1...\n",
      "Epoch 1/5\n",
      "5/5 [==============================] - 72s 12s/step - loss: 1.3527 - binary_accuracy: 0.1447\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 58s 12s/step - loss: 1.1010 - binary_accuracy: 0.2039\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 60s 12s/step - loss: 0.5968 - binary_accuracy: 0.6908\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 60s 12s/step - loss: 0.3657 - binary_accuracy: 0.8816\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 61s 12s/step - loss: 0.3625 - binary_accuracy: 0.8684\n",
      "F1 Score : [0.92957746 0.        ]\n",
      "Training chunk2...\n",
      "Epoch 1/5\n",
      "5/5 [==============================] - 59s 12s/step - loss: 0.3609 - binary_accuracy: 0.8684\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 57s 11s/step - loss: 0.3112 - binary_accuracy: 0.8750\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 57s 11s/step - loss: 0.2739 - binary_accuracy: 0.8882\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 57s 11s/step - loss: 0.2287 - binary_accuracy: 0.9145\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 56s 11s/step - loss: 0.1916 - binary_accuracy: 0.9276\n",
      "F1 Score : [0.97358491 0.82051282]\n",
      "Training chunk3...\n",
      "Epoch 1/5\n",
      "5/5 [==============================] - 56s 11s/step - loss: 0.3841 - binary_accuracy: 0.8553\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 63s 12s/step - loss: 0.2873 - binary_accuracy: 0.8947\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 61s 12s/step - loss: 0.2003 - binary_accuracy: 0.9211\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 61s 12s/step - loss: 0.1263 - binary_accuracy: 0.9605\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 57s 11s/step - loss: 0.0822 - binary_accuracy: 0.9737\n",
      "F1 Score : [0.99242424 0.95      ]\n",
      "Training chunk4...\n",
      "Epoch 1/5\n",
      "5/5 [==============================] - 56s 11s/step - loss: 0.3233 - binary_accuracy: 0.8882\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 56s 11s/step - loss: 0.1832 - binary_accuracy: 0.9211\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 56s 11s/step - loss: 0.0852 - binary_accuracy: 0.9737\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 57s 11s/step - loss: 0.0423 - binary_accuracy: 0.9934\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 57s 11s/step - loss: 0.0088 - binary_accuracy: 1.0000\n",
      "F1 Score : [1. 1.]\n",
      "Training chunk5...\n",
      "Epoch 1/5\n",
      "5/5 [==============================] - 57s 11s/step - loss: 0.5465 - binary_accuracy: 0.8750\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 57s 11s/step - loss: 0.3412 - binary_accuracy: 0.9276\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 57s 11s/step - loss: 0.2881 - binary_accuracy: 0.9145\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 57s 11s/step - loss: 0.1708 - binary_accuracy: 0.9605\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 56s 11s/step - loss: 0.0865 - binary_accuracy: 0.9605\n",
      "F1 Score : [0.98507463 0.88888889]\n",
      "Training chunk6...\n",
      "Epoch 1/5\n",
      "5/5 [==============================] - 56s 11s/step - loss: 0.2545 - binary_accuracy: 0.9079\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 69s 13s/step - loss: 0.1909 - binary_accuracy: 0.9408\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 61s 12s/step - loss: 0.1184 - binary_accuracy: 0.9539\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 64s 13s/step - loss: 0.0559 - binary_accuracy: 0.9803\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 65s 13s/step - loss: 0.0098 - binary_accuracy: 1.0000\n",
      "F1 Score : [1. 1.]\n",
      "Training chunk7...\n",
      "Epoch 1/5\n",
      "5/5 [==============================] - 67s 12s/step - loss: 0.4691 - binary_accuracy: 0.8947\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 57s 11s/step - loss: 0.3537 - binary_accuracy: 0.9211\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 57s 11s/step - loss: 0.2031 - binary_accuracy: 0.9342\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 57s 11s/step - loss: 0.1295 - binary_accuracy: 0.9605\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 57s 11s/step - loss: 0.0816 - binary_accuracy: 0.9803\n",
      "F1 Score : [0.98867925 0.92307692]\n",
      "Training chunk8...\n",
      "Epoch 1/5\n",
      "5/5 [==============================] - 61s 11s/step - loss: 0.1874 - binary_accuracy: 0.9276\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 56s 11s/step - loss: 0.1325 - binary_accuracy: 0.9671\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 60s 12s/step - loss: 0.0867 - binary_accuracy: 0.9737\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 61s 11s/step - loss: 0.0367 - binary_accuracy: 0.9934\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 59s 12s/step - loss: 0.0257 - binary_accuracy: 0.9934\n",
      "F1 Score : [0.99622642 0.97435897]\n",
      "Training chunk9...\n",
      "Epoch 1/5\n",
      "5/5 [==============================] - 56s 11s/step - loss: 0.3303 - binary_accuracy: 0.9276\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 56s 11s/step - loss: 0.1883 - binary_accuracy: 0.9539\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 56s 11s/step - loss: 0.0830 - binary_accuracy: 0.9737\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 56s 11s/step - loss: 0.0771 - binary_accuracy: 0.9671\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 56s 11s/step - loss: 0.0059 - binary_accuracy: 1.0000\n",
      "F1 Score : [1. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x7fdb1c56d5e0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(train_dataframe_collection, bert_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-Anorexic       0.93      0.95      0.94       132\n",
      "    Anorexic       0.65      0.55      0.59        20\n",
      "\n",
      "    accuracy                           0.90       152\n",
      "   macro avg       0.79      0.75      0.77       152\n",
      "weighted avg       0.90      0.90      0.90       152\n",
      "\n"
     ]
    }
   ],
   "source": [
    "validate(val_df, bert_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96       279\n",
      "           1       0.79      0.56      0.66        41\n",
      "\n",
      "    accuracy                           0.93       320\n",
      "   macro avg       0.87      0.77      0.81       320\n",
      "weighted avg       0.92      0.93      0.92       320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_model(test_dataframe_collection, test_truth_labels, bert_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_dev",
   "language": "python",
   "name": "project_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
